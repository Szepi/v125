---
title: Learning Over-Parametrized Two-Layer Neural Networks beyond NTK
abstract: " We consider the dynamic of gradient descent for learning a two-layer neural
  network. We assume the input $x\\in\\mathbb{R}^d$ is drawn from a Gaussian distribution
  and the label of $x$ satisfies $f^{\\star}(x) = a^{\\top}|W^{\\star}x|$, where $a\\in\\mathbb{R}^d$
  is a nonnegative vector and $W^{\\star} \\in\\mathbb{R}^{d\\times d}$ is an orthonormal
  matrix. We show that an \\emph{over-parameterized} two layer neural network with
  ReLU activation, trained by gradient descent from \\emph{random initialization},
  can provably learn the ground truth network with population loss at most $o(1/d)$
  in polynomial time with polynomial samples. On the other hand, we prove that any
  kernel method, including Neural Tangent Kernel, with a polynomial number of samples
  in $d$, has population loss at least $\\Omega(1 / d)$."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: li20a
month: 0
tex_title: Learning Over-Parametrized Two-Layer Neural Networks beyond NTK
firstpage: 2613
lastpage: 2682
page: 2613-2682
order: 2613
cycles: false
bibtex_author: Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R.
author:
- given: Yuanzhi
  family: Li
- given: Tengyu
  family: Ma
- given: Hongyang R.
  family: Zhang
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/li20a/li20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
