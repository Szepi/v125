---
title: Optimality and Approximation with Policy Gradient Methods in Markov Decision
  Processes
abstract: " Policy gradient (PG) methods are among the most effective methods in challenging
  reinforcement learning problems with large state and/or action spaces. However,
  little is known about even their most basic theoretical convergence properties,
  including:  if and how fast they converge to a globally optimal solution (say with
  a sufficiently rich policy class);  how they cope with approximation error due to
  using a restricted class of parametric policies; or  their finite sample behavior.
  \ Such characterizations are important not only to compare these methods to their
  approximate value function counterparts (where such issues are relatively well understood,
  at least in the worst case), but also to help with more principled approaches to
  algorithm design. This work provides provable characterizations of computational,
  approximation, and sample size issues with regards to policy gradient methods in
  the context of discounted Markov Decision Processes (MDPs). We focus on both: 1)
  “tabular” policy parameterizations, where the optimal policy is contained in the
  class and where we show global convergence to the optimal policy, and 2) restricted
  policy classes, which may not contain the optimal policy and where we provide agnostic
  learning results.  In the \\emph{tabular setting}, our main results are: 1) convergence
  rate to global optimum for direct parameterization and projected gradient ascent
  \ 2) an asymptotic convergence to global optimum for softmax policy parameterization
  and PG; and a convergence rate with additional entropy regularization, and 3) dimension-free
  convergence to global optimum for softmax policy parameterization and Natural Policy
  Gradient (NPG) method with exact gradients. In \\emph{function approximation}, we
  further analyze NPG with exact as well as inexact gradients under certain smoothness
  assumptions on the policy parameterization and establish rates of convergence in
  terms of the quality of the initial state distribution. One insight of this work
  is in formalizing how a favorable initial state distribution provides a means to
  circumvent worst-case exploration issues.  Overall, these results place PG methods
  under a solid theoretical footing, analogous to the global convergence guarantees
  of iterative value function based algorithms."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: agarwal20a
month: 0
tex_title: Optimality and Approximation with Policy Gradient Methods in Markov Decision
  Processes
firstpage: 64
lastpage: 66
page: 64-66
order: 64
cycles: false
bibtex_author: Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav
author:
- given: Alekh
  family: Agarwal
- given: Sham M
  family: Kakade
- given: Jason D
  family: Lee
- given: Gaurav
  family: Mahajan
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/agarwal20a/agarwal20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
