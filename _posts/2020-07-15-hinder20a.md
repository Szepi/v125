---
title: Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond
abstract: ' In this paper, we provide near-optimal accelerated first-order methods
  for minimizing a broad class of smooth nonconvex functions that are unimodal on
  all lines through a minimizer. This function class, which we call the class of smooth
  quasar-convex functions, is parameterized by a constant $$\gamma \in (0,1]$$: $$\gamma
  = 1$$ encompasses the classes of smooth convex and star-convex functions, and smaller
  values of $$\gamma$$ indicate that the function can be "more nonconvex." We develop
  a variant of accelerated gradient descent that computes an $$\epsilon$$-approximate
  minimizer of a smooth $$\gamma$$-quasar-convex function with at most $$O(\gamma^{-1}
  \epsilon^{-1/2} \log(\gamma^{-1} \epsilon^{-1}))$$ total function and gradient evaluations.
  We also derive a lower bound of $$\Omega(\gamma^{-1} \epsilon^{-1/2})$$ on the worst-case
  number of gradient evaluations required by any deterministic first-order method,
  showing that, up to a logarithmic factor, no deterministic first-order method can
  improve upon ours.'
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hinder20a
month: 0
tex_title: Near-Optimal Methods for Minimizing Star-Convex Functions and Beyond
firstpage: 1894
lastpage: 1938
page: 1894-1938
order: 1894
cycles: false
bibtex_author: Hinder, Oliver and Sidford, Aaron and Sohoni, Nimit
author:
- given: Oliver
  family: Hinder
- given: Aaron
  family: Sidford
- given: Nimit
  family: Sohoni
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/hinder20a/hinder20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
