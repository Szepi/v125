---
title: Gradient descent follows the regularization path for general losses
abstract: " Recent work across many machine learning disciplines has highlighted that
  standard descent methods, even without explicit regularization, do not merely minimize
  the training error, but also exhibit an \\emph{implicit bias}. This bias is typically
  towards a certain regularized solution, and relies upon the details of the learning
  process, for instance the use of the cross-entropy loss. In this work, we show that
  for empirical risk minimization over linear predictors with \\emph{arbitrary} convex,
  strictly decreasing losses, if the risk does not attain its infimum, then the gradient-descent
  path and the \\emph{algorithm-independent} regularization path converge to the same
  direction (whenever either converges to a direction). Using this result, we provide
  a justification for the widely-used exponentially-tailed losses (such as the exponential
  loss or the logistic loss): while this convergence to a direction for exponentially-tailed
  losses is necessarily to the maximum-margin direction, other losses such as polynomially-tailed
  losses may induce convergence to a direction with a poor margin. "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ji20a
month: 0
tex_title: Gradient descent follows the regularization path for general losses
firstpage: 2109
lastpage: 2136
page: 2109-2136
order: 2109
cycles: false
bibtex_author: Ji, Ziwei and Dud{\'i}k, Miroslav and Schapire, Robert E. and Telgarsky,
  Matus
author:
- given: Ziwei
  family: Ji
- given: Miroslav
  family: Dud√≠k
- given: Robert E.
  family: Schapire
- given: Matus
  family: Telgarsky
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/ji20a/ji20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
