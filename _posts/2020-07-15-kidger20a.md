---
title: Universal Approximation with Deep Narrow Networks
abstract: " The classical Universal Approximation Theorem holds for neural networks
  of arbitrary width and bounded depth. Here we consider the natural ‘dual’ scenario
  for networks of bounded width and arbitrary depth. Precisely, let $n$ be the number
  of inputs neurons, $m$ be the number of output neurons, and let $\\rho$ be any nonaffine
  continuous function, with a continuous nonzero derivative at some point. Then we
  show that the class of neural networks of arbitrary depth, width $n + m + 2$, and
  activation function $\\rho$, is dense in $C(K; \\mathbb{R}^m)$ for $K \\subseteq
  \\mathbb{R}^n$ with $K$ compact. This covers every activation function possible
  to use in practice, and also includes polynomial activation functions, which is
  unlike the classical version of the theorem, and provides a qualitative difference
  between deep narrow networks and shallow wide networks. We then consider several
  extensions of this result. In particular we consider nowhere differentiable activation
  functions, density in noncompact domains with respect to the $L^p$-norm, and how
  the width may be reduced to just $n + m + 1$ for ‘most’ activation functions."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kidger20a
month: 0
tex_title: "{Universal Approximation with Deep Narrow Networks}"
firstpage: 2306
lastpage: 2327
page: 2306-2327
order: 2306
cycles: false
bibtex_author: Kidger, Patrick and Lyons, Terry
author:
- given: Patrick
  family: Kidger
- given: Terry
  family: Lyons
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/kidger20a/kidger20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
