---
title: Efficient improper learning for online logistic regression
abstract: " We consider the setting of online  logistic regression and consider the
  regret with respect to the $\\ell_2$-ball of radius $B$. It is known (see Hazan
  et al. (2014)) that any proper algorithm which has logarithmic regret in the number
  of samples (denoted $n$) necessarily suffers an exponential multiplicative constant
  in $B$. In this work, we design an efficient improper algorithm that avoids this
  exponential constant while preserving a logarithmic regret. Indeed, Foster et al.
  (2018) showed that the lower bound  does not apply to improper algorithms and proposed
  a strategy based on exponential weights with prohibitive computational complexity.
  Our new algorithm based on regularized empirical risk minimization with surrogate
  losses satisfies a regret scaling as $O(B\\log(Bn))$ with a per-round time-complexity
  of order $O(d^2 + \\log(n))$."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: jezequel20a
month: 0
tex_title: Efficient improper learning for online logistic regression
firstpage: 2085
lastpage: 2108
page: 2085-2108
order: 2085
cycles: false
bibtex_author: J{\'e}z{\'e}quel, R{\'e}mi and Gaillard, Pierre and Rudi, Alessandro
author:
- given: Rémi
  family: Jézéquel
- given: Pierre
  family: Gaillard
- given: Alessandro
  family: Rudi
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/jezequel20a/jezequel20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
