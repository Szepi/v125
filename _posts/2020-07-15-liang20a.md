---
title: On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower Isometry
  of Kernels
abstract: " We study the risk of minimum-norm interpolants of data in Reproducing
  Kernel Hilbert Spaces. Our upper bounds on the risk are of a multiple-descent shape
  for the various scalings of $d = n^{\\alpha}$, $\\alpha\\in(0,1)$, for the input
  dimension $d$ and sample size $n$. Empirical evidence supports our finding that
  minimum-norm interpolants in RKHS can exhibit this unusual non-monotonicity in sample
  size; furthermore,  locations of the peaks in our experiments match our theoretical
  predictions. Since gradient flow on appropriately initialized wide neural networks
  converges to a  minimum-norm interpolant with respect to a certain kernel, our analysis
  also yields novel estimation and generalization guarantees for these over-parametrized
  models. At the heart of our analysis is a study of spectral properties of the random
  kernel matrix restricted to a filtration of eigen-spaces of the population covariance
  operator, and may be of independent interest.  "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: liang20a
month: 0
tex_title: On the Multiple Descent of Minimum-Norm Interpolants and Restricted Lower
  Isometry of Kernels
firstpage: 2683
lastpage: 2711
page: 2683-2711
order: 2683
cycles: false
bibtex_author: Liang, Tengyuan and Rakhlin, Alexander and Zhai, Xiyu
author:
- given: Tengyuan
  family: Liang
- given: Alexander
  family: Rakhlin
- given: Xiyu
  family: Zhai
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/liang20a/liang20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
