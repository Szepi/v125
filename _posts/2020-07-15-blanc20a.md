---
title: Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck
  like process
abstract: " We consider networks, trained via stochastic gradient descent to minimize
  $\\ell_2$ loss, with the training labels perturbed by independent noise at each
  iteration. We characterize the behavior of the training dynamics near any parameter
  vector that achieves zero training error, in terms of an implicit regularization
  term corresponding to the sum over the data points, of the squared $\\ell_2$ norm
  of the gradient of the model with respect to the parameter vector, evaluated at
  each data point.  This holds for networks of any connectivity, width, depth, and
  choice of activation function.  We interpret this implicit regularization term for
  three simple settings: matrix sensing, two layer ReLU networks trained on one-dimensional
  data, and two layer networks with sigmoid activations trained on a single datapoint.
  \ For these settings, we show why this new and general implicit regularization effect
  drives the networks towards “simple” models. "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: blanc20a
month: 0
tex_title: Implicit regularization for deep neural networks driven by an Ornstein-Uhlenbeck
  like process
firstpage: 483
lastpage: 513
page: 483-513
order: 483
cycles: false
bibtex_author: Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul
author:
- given: Guy
  family: Blanc
- given: Neha
  family: Gupta
- given: Gregory
  family: Valiant
- given: Paul
  family: Valiant
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/blanc20a/blanc20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
