---
title: Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained
  with the Logistic Loss
abstract: " Neural networks trained to minimize the logistic (a.k.a. cross-entropy)
  loss with gradient-based methods are observed to perform well in many supervised
  classification tasks. Towards understanding this phenomenon, we analyze the training
  and generalization behavior of infinitely wide two-layer neural networks with homogeneous
  activations. We show that the limits of the gradient flow on exponentially tailed
  losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian
  space of functions. In presence of hidden low-dimensional structures, the resulting
  margin is independent of the ambiant dimension, which leads to strong generalization
  bounds. In contrast, training only the output layer implicitly solves a kernel support
  vector machine, which a priori does not enjoy such an adaptivity. Our analysis of
  training is non-quantitative in terms of running time but we prove computational
  guarantees in simplified settings by showing equivalences with online mirror descent.
  Finally, numerical experiments suggest that our analysis describes well the practical
  behavior of two-layer neural networks with ReLU activation and confirm the statistical
  benefits of this implicit bias."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: l-enaic20a
month: 0
tex_title: Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained
  with the Logistic Loss
firstpage: 1305
lastpage: 1338
page: 1305-1338
order: 1305
cycles: false
bibtex_author: L'ena\"ic, Chizat and Francis, Bach
author:
- given: L’enaïc
  family: Chizat
- given: Francis
  family: Bach
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---

