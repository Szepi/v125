---
title: 'Open Problem: Tight Convergence of SGD in Constant Dimension'
abstract: Stochastic Gradient Descent (SGD) is one of the most popular optimization
  methods in machine learning and has been studied extensively since the early 50â€™s.
  However, our understanding of this fundamental algorithm is still lacking in certain
  aspects. We point out to a gap that remains between the known upper and lower bounds
  for the expected suboptimality of the last SGD point whenever the dimension is a
  constant independent of the number of SGD iterations $T$, and in particular, that
  the gap is still unaddressed even in the one dimensional case. For the latter, we
  provide evidence that the correct rate is $\Theta(1/\sqrt{T})$ and conjecture that
  the same applies in any (constant) dimension.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: koren20a
month: 0
tex_title: 'Open Problem: Tight Convergence of SGD in Constant Dimension'
firstpage: 3847
lastpage: 3851
page: 3847-3851
order: 3847
cycles: false
bibtex_author: Koren, Tomer and Segal, Shahar
author:
- given: Tomer
  family: Koren
- given: Shahar
  family: Segal
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/koren20a/koren20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
