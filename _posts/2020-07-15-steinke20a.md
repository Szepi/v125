---
title: Reasoning About Generalization via Conditional Mutual Information
abstract: " We provide an information-theoretic framework for studying the generalization
  properties of machine learning algorithms. Our framework ties together existing
  approaches, including uniform convergence bounds and recent methods for adaptive
  data analysis. Specifically, we use Conditional Mutual Information (CMI) to quantify
  how well the input (i.e., the training data) can be recognized given the output
  (i.e., the trained model) of the learning algorithm. We show that bounds on CMI
  can be obtained from VC dimension, compression schemes, differential privacy, and
  other methods. We then show that bounded CMI implies various forms of generalization."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: steinke20a
month: 0
tex_title: "{R}easoning {A}bout {G}eneralization via {C}onditional {M}utual {I}nformation"
firstpage: 3437
lastpage: 3452
page: 3437-3452
order: 3437
cycles: false
bibtex_author: Steinke, Thomas and Zakynthinou, Lydia
author:
- given: Thomas
  family: Steinke
- given: Lydia
  family: Zakynthinou
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/steinke20a/steinke20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
