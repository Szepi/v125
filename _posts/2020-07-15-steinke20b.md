---
title: 'Open Problem: Information Complexity of VC Learning'
abstract: Uniform convergence approaches learning by studying the complexity of hypothesis
  classes. In particular, hypothesis classes with bounded Vapnik-Chervonenkis dimension
  exhibit strong uniform convergence, that is, any hypothesis in the class has low
  generalization error. On the other hand, a long line of work studies the information
  complexity of a learning algorithm, as it is connected to several desired properties,
  including generalization.  We ask whether all VC classes admit a learner with low
  information complexity which achieves the generalization bounds guaranteed by uniform
  convergence. Specifically, since we know that this is not possible if we consider
  proper and consistent learners and measure information complexity in terms of the
  mutual information (Bassily et al., 2018), we are interested in learners with low
  information complexity measured in terms of the recently introduced notion of CMI
  (Steinke and Zakynthinou, 2020). Can we obtain tight bounds on the information complexity
  of a learning algorithm for a VC class (via CMI), thus exactly retrieving the known
  generalization bounds implied for this class by uniform convergence?
layout: inproceedings
series: Proceedings of Machine Learning Research
id: steinke20b
month: 0
tex_title: "{O}pen {P}roblem: {I}nformation {C}omplexity of {VC} {L}earning"
firstpage: 3857
lastpage: 3863
page: 3857-3863
order: 3857
cycles: false
bibtex_author: Steinke, Thomas and Zakynthinou, Lydia
author:
- given: Thomas
  family: Steinke
- given: Lydia
  family: Zakynthinou
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/steinke20b/steinke20b.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
