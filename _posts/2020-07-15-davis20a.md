---
title: High probability guarantees for stochastic convex optimization
abstract: " Standard results in stochastic convex optimization bound the number of
  samples that an algorithm needs to generate a point with small function value in
  expectation. More nuanced high probability guarantees are rare, and typically either
  rely on “light-tail” noise assumptions or exhibit worse sample complexity. In this
  work, we show that a wide class of stochastic optimization algorithms for strongly
  convex problems can be augmented with high confidence bounds at an overhead cost
  that is only logarithmic in the confidence level and polylogarithmic in the condition
  number. The procedure we propose, called proxBoost, is elementary and builds on
  two well-known ingredients: robust distance estimation and the proximal point method.
  We discuss consequences for both streaming (online) algorithms and offline algorithms
  based on empirical risk minimization."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: davis20a
month: 0
tex_title: High probability guarantees for stochastic convex optimization
firstpage: 1411
lastpage: 1427
page: 1411-1427
order: 1411
cycles: false
bibtex_author: Davis, Damek and Drusvyatskiy, Dmitriy
author:
- given: Damek
  family: Davis
- given: Dmitriy
  family: Drusvyatskiy
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/davis20a/davis20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
