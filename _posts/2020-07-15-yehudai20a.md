---
title: Learning a Single Neuron with Gradient Methods
abstract: " We consider the fundamental problem of learning a single neuron $\\mathbf{x}\\mapsto
  \\sigma(\\mathbf{w}^\\top\\mathbf{x})$ in a realizable setting, using standard gradient
  methods with random initialization, and under general families of input distributions
  and activations. On the one hand, we show that some assumptions on both the distribution
  and the activation function are necessary. On the other hand, we prove positive
  guarantees under mild assumptions, which go significantly beyond those studied in
  the literature so far. We also point out and study the challenges in further strengthening
  and generalizing our results."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: yehudai20a
month: 0
tex_title: Learning a Single Neuron with Gradient Methods
firstpage: 3756
lastpage: 3786
page: 3756-3786
order: 3756
cycles: false
bibtex_author: Yehudai, Gilad and Ohad, Shamir
author:
- given: Gilad
  family: Yehudai
- given: Shamir
  family: Ohad
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/yehudai20a/yehudai20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
