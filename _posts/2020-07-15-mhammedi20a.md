---
title: Lipschitz and Comparator-Norm Adaptivity in Online Learning
abstract: " We study Online Convex Optimization in the unbounded setting where neither
  predictions nor gradient are constrained. The goal is to simultaneously adapt to
  both the sequence of gradients and the comparator. We first develop parameter-free
  and scale-free algorithms for a simplified setting with hints. We present two versions:
  the first adapts to the squared norms of both comparator and gradients separately
  using $O(d)$ time per round, the second adapts to their squared inner products (which
  measure variance only in the comparator direction) in time $O(d^3)$ per round. We
  then generalize two prior reductions to the unbounded setting; one to not need hints,
  and a second to deal with the range ratio problem (which already arises in prior
  work). We discuss their optimality in light of prior and new lower bounds. We apply
  our methods to obtain sharper regret bounds for scale-invariant online prediction
  with linear models. "
layout: inproceedings
series: Proceedings of Machine Learning Research
id: mhammedi20a
month: 0
tex_title: Lipschitz and Comparator-Norm Adaptivity in Online Learning
firstpage: 2858
lastpage: 2887
page: 2858-2887
order: 2858
cycles: false
bibtex_author: Mhammedi, Zakaria and Koolen, Wouter M.
author:
- given: Zakaria
  family: Mhammedi
- given: Wouter M.
  family: Koolen
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/mhammedi20a/mhammedi20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
