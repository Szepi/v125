---
title: The Gradient Complexity of Linear Regression
abstract: " We investigate the computational complexity of several basic linear algebra
  primitives, including largest eigenvector computation and linear regression, in
  the computational model that allows access to the data via a matrix-vector product
  oracle. We show that for polynomial accuracy, $\\Theta(d)$ calls to the oracle are
  necessary and sufficient even for a randomized algorithm. Our lower bound is based
  on a reduction to estimating the least eigenvalue of a random Wishart matrix. This
  simple distribution enables a concise proof, leveraging a few key properties of
  the random Wishart ensemble."
layout: inproceedings
series: Proceedings of Machine Learning Research
id: braverman20a
month: 0
tex_title: The Gradient Complexity of Linear Regression
firstpage: 627
lastpage: 647
page: 627-647
order: 627
cycles: false
bibtex_author: Braverman, Mark and Hazan, Elad and Simchowitz, Max and Woodworth,
  Blake
author:
- given: Mark
  family: Braverman
- given: Elad
  family: Hazan
- given: Max
  family: Simchowitz
- given: Blake
  family: Woodworth
date: 2020-07-15
address: 
publisher: PMLR
container-title: Proceedings of Thirty Third Conference on Learning Theory
volume: '125'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 7
  - 15
pdf: http://proceedings.mlr.press/v125/braverman20a/braverman20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
